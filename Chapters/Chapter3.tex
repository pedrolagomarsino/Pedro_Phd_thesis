% Chapter 2

\chapter{Materials and Methods} % Main chapter title

\label{Chapter3} % For referencing the chapter elsewhere, use \ref{Chapter2} 

% =========================================================== %
%          Subsection: Experimental procedures                %
% =========================================================== %
\section{Experimental procedures}
\label{chap3:sec:1:exp_proc}
How was the experiment performed? (briefly) animals, surgeries, 1p/2p imaging.

% =========================================================== %
%          Subsection: Data pre-processing                    %
% =========================================================== %
\section{Data pre-processing}
\label{chap3:sec:2:preproc}
In this work we dealt with diverse and complex datasets, of different types and structures and therefore, that require different pre-processing pipelines. 
Here the details for each case.
\subsection{Inscopix 1-photon imaging}
\label{chap3:sec:2:subsec1:inscopix-pre-proc}
The Inscopix software acquires imaging data from miniscopes in freely moving animals. The imaging data is then exported as \textit{.isxd} files containing the images and the metadata. 
Such files can only be read and treated with Inscopix own proprietary software (\textbf{poner el link}), the following pre-processing was performed using such software. 

All five imaging series corresponding to the same animal in the same day were first concatenated, cropped and downsampled in space and time. 
Temporal downsampling works by averaging $n$ adjacent frames, where $n$ is the temporal downsample factor. 
The moving average stride is equal to the temporal downsample factor, which results in non-overlapping groups of frames to be averaged. 
This is equal to binning the frame data in time (in bins defined by the temporal downsample factor) and the subsequent averaging of each bin. 
The resulting number of frames equals the original number of frames divided by the temporal downsample factor, rounded down. 
Spatial downsampling works similarly, except that the spatial bins are non-overlapping sub-images of the original frames.
For all recordings we used a temporal and spatial downsample factor of 2 and 4 respectively.  
Both downsampling stages were used to be able to realistically mange data size and computation time. \\
To remove defective pixels a 3x3 median filter was applied to the movies, and early frames which were dark or dim were trimmed.\\  
A spatial filter algorithm was then applied to each movie to remove low and high spatial frequency content. In practice the algorithm bandpass the images by convolving each frame with a gaussian kernel and subtracting a smoothed version of the frame from a less smoothed version of the frame.
Parameters of the bandpass filter were set to \textbf{Low cut-off} $ = 0.005 \ {pixel}^{-1}$ and a  \textbf{High cut-off} $ = 0.5 \ {pixel}^{-1}$. \\
Then each concatenated recording was motion corrected to compensate for unwanted motion of the brain relative to the skull.
For each frame of the movie, motion correction estimates a translation that minimizes the difference between the transformed frame and the reference frame, using an image registration method described in \textit{REF [Thevenaz1998]}. \\
Then the fluorescence in each pixel was normalize by the average fluorescence across frames to obtain the $\Delta F/F$, so that it represents a deviation or change from a baseline.  
\subsection{2-photon imaging}
\label{chap3:sec:2:subsec1:2p-pre-proc}
Data extracted using 2-photon microscopy produces t-series consisting of sequential \textit{.tiff} images. 
All images corresponding to a t-series were first concatenated to produce an \textit{.avi} video with no compression. \\
Motion corrected was then performed using the \textit{NoRMCorre} algorithm \textit{(REF Pnevmatikakis and Giovannucci, 2017)}, that corrects non-rigid motion artifacts by estimating motion vectors with subpixel resolution over a set of overlapping patches within the FOV. 
These estimates are used to infer a smooth motion field within the FOV for each frame. 
The inferred motion fields are then applied to the original data frames.\\
Motion correction was applied in two steps, first each t-series was motion corrected.
Then, all t-series corresponding to the same day and same animal were concatenated and motion corrected again. 

% =========================================================== %
%            Subsection: Animal tracking                      %
% =========================================================== %
\section{Animal tracking}
\label{chap3:sec:3:tracking}
\subsection{Linear track}
\label{chap3:sec:3:subsec1:linear-track-tracking}
Animals performing the linear track task were head fixed and able to run in a wheel.
The position of the animal is then considered as the position of the avatar in the virtual reality linear track. \textbf{PONER LA PARTE DE LOS METODOS DE SEBA}
\subsection{2-D arena}
\label{chap3:sec:3:subsec2:2d-arena-tracking}
In the random foraging and open field experiments in the 2-dimensional arena animals were free to move and explore a $45 cm x 45 cm$ square box. The box was filmed using a \textbf{CAMARA MODEL} placed at \textbf{DISTANCE} meters from the floor. \\
Animal position was estimated from these videos using the software package \textbf{DeepLabCut} (DLC) \textbf{Link to the githubpage}.\\
DLC is a free software 000000000000000000000000000000000000000


% =========================================================== %
%          Subsection: Video Segmentation                     %
% =========================================================== %
\section{Video Segmentation}
\label{chap3:sec:4:segmentation}

% =========================================================== %
%               Subsection: Trace extraction                  %
% =========================================================== %
\section{Trace extraction}
\label{chap3:sec:5:trace_extraction}

% =========================================================== %
%          Subsection: Event detection                        %
% =========================================================== %
\section{Event detection}
\label{chap3:sec:6:event_det}

% =========================================================== %
%          Subsection: Place Cell detection                   %
% =========================================================== %
\section{Place Cell detection}
\label{chap3:sec:7:pc_det}

% =========================================================== %
%          Subsection: Statistical testing                    %
% =========================================================== %
\section{Statistical testing}
\label{chap3:sec:8:stats}

% =========================================================== %
%   Subsection: Decoding of position from neural activity     %
% =========================================================== %
\section{Decoding of position from neural activity}
\label{chap3:sec:9:decoders}

% =========================================================== %
%          Subsection: Dimensionality reduction               %
% =========================================================== %
\section{Dimensionality reduction}
\label{chap3:sec:10:dim_red}
